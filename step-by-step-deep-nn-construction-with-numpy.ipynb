{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Author: Fatih Can Akıncı\n# twitter/github/kaggle: akincifca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0714e6034399b8c95e346a1a455dab79990f12e1"},"cell_type":"code","source":"# First of all, I want to thank to Professor Murat Karakaya \n# for giving me the opportunity to dive deep into deep learning and\n# motivate me to push my limits\n\n# This is a tutorial for introduction to deep neural networks.\n# In this notebook, I tried to construct the network layer by layer\n# and tried to understand the underlying mathematics in deep learning\n# libraries such as Keras by implementing the network with pure numpy.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c628c4fe79e57aa83d579fd86bdb55f006213a"},"cell_type":"code","source":"# First import Numpy\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"224a33bbd762986925066d7db12f5e88ee8ef13b"},"cell_type":"code","source":"# Activation Functions for neural network nodes\n# Sigmoid activation function\ndef sigmoid(x):\n    return 1.0/(1+ np.exp(-x))\n  \n# Derivative of sigmoid funcion\ndef sigmoid_derivative(x):\n    return x * (1.0 - x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa6c27d8af036558e8d9c2ae5caf2893df0b3d0"},"cell_type":"code","source":"# Lets design the network:\nclass Dense:\n    \n    def __init__(self, x, nodes, activation, lr=0.05): \n        self.inputs = x\n        self.nodes = nodes\n        self.activation = activation\n        self.lr = lr\n        # Initialize weights\n        self.weights = np.random.normal(0.0, pow(self.inputs.shape[0], -0.5), (self.nodes, self.inputs.shape[0]))\n        pass\n    \n    def feedforward(self):\n        self.output = self.activation(np.dot(self.weights, self.inputs))    \n        return self.output\n    \n    def backprop(self, prev_error, prev_w, activation_der):\n        self.activation_der = activation_der\n        self.error = np.dot(prev_w.T, prev_error)\n        # application of the chain rule to find derivative of the loss function with respect to weights\n        self.d_weights = np.dot((self.error * self.activation_der(self.output)), self.inputs.T)\n        # print(d_weights)\n        # update the weights with the derivative (slope) of the loss function\n        self.weights += self.lr * self.d_weights     \n        return self.d_weights\n\nclass Output:\n    \n    def __init__(self, x, nodes, activation, lr=0.05):\n        self.inputs = x\n        self.nodes = nodes\n        self.activation = activation\n        self.lr = lr\n        # Initialize weights\n        self.weights = np.random.normal(0.0, pow(self.inputs.shape[0], -0.5), (self.nodes, self.inputs.shape[0]))\n        pass\n\n    def feedforward(self):\n\n        self.output = self.activation(np.dot(self.weights, self.inputs))\n        return self.output\n\n    def backprop(self, y, activation_der):\n\n        self.y = y\n        self.activation_der = activation_der\n        # error\n        self.output_error = self.y - self.output\n        # application of the chain rule to find derivative of the loss function with respect to weights\n        self.d_weights = np.dot((self.output_error * self.activation_der(self.output)), self.inputs.T)\n        # print(d_weights)\n        # update the weights with the derivative (slope) of the loss function\n        self.weights += self.lr * self.d_weights\n        return self.d_weights\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a75b7234769179dc8abf87c776bb71348b974977"},"cell_type":"code","source":"# Step-by-Step Example of forward prop and back prop\n# Lets take an input \"X\" and a target \"y\" as follows:\nX = np.array([[0.2,0.1,0.3]]).T\ny = np.array([[0.8,0.5,0.6]]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33eb988ccf43a4fa8c3801277acf2187ed40edc0"},"cell_type":"code","source":"# Construct the NN\n# Lets create our hidden layer that has 5 nodes\nhiddenLayer = Dense(X, 5, sigmoid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa5673898c959b243b579235fb308452904c8883"},"cell_type":"code","source":"# Check our hidden layers random initialized weights\nhiddenLayer.weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9411ac9de95d80f329e73c7aced68e53cc86bd7"},"cell_type":"code","source":"# Feedforward the layer to get output values from hidden layer\nhiddenLayer.feedforward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c9b869ff617a568798816b9b43652830e65a35f"},"cell_type":"code","source":"# Lets create output layer which has also 3 nodes\noutputLayer = Output(hiddenLayer.output, 3, sigmoid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d08f48370d38f6d8d56a9cb7b6a2d4377b09f34e"},"cell_type":"code","source":"# Feedforward output layer and make predictions to find \"y\"\noutputLayer.feedforward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5469af0f21a969930e421fdf63d96d5a7d67fba5"},"cell_type":"code","source":"# Our target values were 0.8, 0.5 and 0.6\n# Not bad but lets apply gradient descent and backprop our neural network to adjust the weights\ndWo = outputLayer.backprop(y, sigmoid_derivative)\nprint(\"dWoutput:\", dWo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b5d09bef40e61c0c1c956568b82cb90749204c"},"cell_type":"code","source":"# backprop again and adjust the hidden layer weights\ndWh = hiddenLayer.backprop(outputLayer.output_error, outputLayer.weights, sigmoid_derivative)\nprint(\"dWhidden:\", dWh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8fda817506d91c34bd6ac477ec93b88116d724"},"cell_type":"code","source":"# Lets see the new weights for hidden layer\nhiddenLayer.weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"831c17c64c991ee87a4f28f6f97a4609ed7b25b3"},"cell_type":"code","source":"# Lets make a new prediction with updated weights\nhiddenLayer.feedforward()\noutputLayer.feedforward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7b8f4e56f02e75ab7c729b7c39adbd561631322"},"cell_type":"code","source":"# Training the network\nfor i in range (1000):\n    outputLayer.backprop(y, sigmoid_derivative)\n    hiddenLayer.backprop(outputLayer.output_error, outputLayer.weights, sigmoid_derivative)\n    hiddenLayer.feedforward()\n    prediction = outputLayer.feedforward()\nprint(\"Prediction after training:\")\nprint(prediction)\nprint(\"Target values:\")\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a1c12024ec7c6ceadfa12f7b2de7a5e0a4b4953"},"cell_type":"code","source":"# Things to do\n# Apply Regression dataset to this NN and adjust parameters\n# Plot accuracy or error during training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e274c0af7a85dd491d22cc835b67b6d0ec9af30b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}